{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Laplacian Pyramid\"\n",
    "\n",
    "This notebook is ment to demonstrate my objections concerning the implementaiton of the laplacian pyramid in DRLN.\n",
    "Please remember their proposed desing of the attention mechanism in their paper. They aim to produce attention differenlty at each pyramid by using different downsampling layers.\n",
    "\n",
    "![Attention](figs/LapAtt.png)\n",
    "In the following cell is the original code of their implementation of the attention layer (CA). As you can see they used three different streames (c1,c2,c3) which are ment to be their \"pyramids\". However, the only parameters they change is the dilation and padding parameter of a 2D convolution as you can see in the BaiscBlock class they use.\n",
    "\n",
    "We need to install create a new conda environment becaus we need a newer version of python due to some bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec trytorch in /home/charlie/.local/share/jupyter/kernels/trytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaValueError: prefix already exists: /home/charlie/anaconda3/envs/trytorch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda env create -f torch.yml # may take a while\n",
    "python -m ipykernel install --user --name=trytorch # activating environment for jupyter notebook\n",
    "# you may need to close and open the notebook again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CALayer, self).__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.c1 = ops.BasicBlock(channel , channel // reduction, 3, 1, 3, 3)\n",
    "        self.c2 = ops.BasicBlock(channel , channel // reduction, 3, 1, 5, 5)\n",
    "        self.c3 = ops.BasicBlock(channel , channel // reduction, 3, 1, 7, 7)\n",
    "        self.c4 = ops.BasicBlockSig((channel // reduction)*3, channel , 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape, \"input\")\n",
    "        y = self.avg_pool(x)\n",
    "        print(y.shape, \"after pooling\")\n",
    "        c1 = self.c1(y)\n",
    "        c2 = self.c2(y)\n",
    "        c3 = self.c3(y)\n",
    "        print(c1.shape, c2.shape, c3.shape, \"shapes\")\n",
    "        c_out = torch.cat([c1, c2, c3], dim=1)\n",
    "        y = self.c4(c_out)\n",
    "        return x * y\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels, out_channels,\n",
    "                 ksize=3, stride=1, pad=1, dilation=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, ksize, stride, pad, dilation),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        init_weights(self.modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But with a input of size (batch_size, channels,\n",
    "1, 1), padding it with the exact amount of zeros needed to fit the\n",
    "increased dilation will render all weights of the convolution layers\n",
    "except the center ones useless. Thus, it seems to me that they do the\n",
    "same computational process in each pyramid which is also the same as the\n",
    "simple CA used by RCAN but with introducing unnecessary weights (i.e.\n",
    "weights that always only get the input zero from the padding) by\n",
    "changing padding and dilation. I tried it myself, and came to the\n",
    "conclusion that one could produce the same result with just using 1x1\n",
    "kernels without padding and dilation, thus getting rid of quite a few\n",
    "parameters.\n",
    "I tried to produce a simple visualization of my remark (see picture below) and wrote the following code to support my claim.\n",
    "![](figs/DRLN_cnn_dilation_and_padding.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the different layers in the Attention Model\n",
    "channels_in = 64\n",
    "reduction = 16\n",
    "channels_out = channels_in // reduction\n",
    "dilation3 = nn.Conv2d(channels_in, channels_out, 3,1,3,3, bias=False)\n",
    "dilation5 = nn.Conv2d(channels_in, channels_out, 3,1,5,5, bias=False)\n",
    "dilation7 = nn.Conv2d(channels_in, channels_out, 3,1,7,7, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[-0.0296, -0.0160, -0.0261],\n",
      "          [ 0.0074, -0.0366, -0.0329],\n",
      "          [-0.0074, -0.0326,  0.0283]],\n",
      "\n",
      "         [[-0.0049,  0.0338, -0.0197],\n",
      "          [ 0.0191,  0.0405, -0.0276],\n",
      "          [ 0.0042, -0.0380,  0.0201]],\n",
      "\n",
      "         [[ 0.0308, -0.0239, -0.0062],\n",
      "          [ 0.0298, -0.0039, -0.0342],\n",
      "          [-0.0376, -0.0120, -0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0155,  0.0216,  0.0206],\n",
      "          [ 0.0246,  0.0229, -0.0130],\n",
      "          [ 0.0151,  0.0111, -0.0261]],\n",
      "\n",
      "         [[-0.0092,  0.0286,  0.0212],\n",
      "          [ 0.0213,  0.0349, -0.0008],\n",
      "          [ 0.0405, -0.0013,  0.0061]],\n",
      "\n",
      "         [[ 0.0008,  0.0283,  0.0324],\n",
      "          [ 0.0035,  0.0311, -0.0252],\n",
      "          [-0.0416, -0.0292, -0.0342]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0067,  0.0026,  0.0203],\n",
      "          [ 0.0078,  0.0066,  0.0337],\n",
      "          [-0.0021, -0.0178,  0.0308]],\n",
      "\n",
      "         [[-0.0322, -0.0343, -0.0317],\n",
      "          [ 0.0384, -0.0275, -0.0406],\n",
      "          [ 0.0344, -0.0247,  0.0343]],\n",
      "\n",
      "         [[ 0.0236, -0.0074, -0.0207],\n",
      "          [ 0.0039,  0.0269, -0.0017],\n",
      "          [ 0.0363,  0.0141, -0.0084]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0330,  0.0308, -0.0370],\n",
      "          [ 0.0162, -0.0342,  0.0173],\n",
      "          [-0.0171, -0.0265,  0.0171]],\n",
      "\n",
      "         [[ 0.0314, -0.0046,  0.0138],\n",
      "          [ 0.0198, -0.0185,  0.0251],\n",
      "          [-0.0214,  0.0346, -0.0119]],\n",
      "\n",
      "         [[-0.0091,  0.0086, -0.0251],\n",
      "          [-0.0348, -0.0267, -0.0308],\n",
      "          [-0.0109,  0.0251, -0.0043]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0041, -0.0346,  0.0407],\n",
      "          [ 0.0061, -0.0016, -0.0334],\n",
      "          [-0.0381,  0.0283, -0.0043]],\n",
      "\n",
      "         [[-0.0004, -0.0098,  0.0028],\n",
      "          [-0.0157,  0.0108, -0.0092],\n",
      "          [ 0.0131,  0.0381, -0.0117]],\n",
      "\n",
      "         [[ 0.0141,  0.0389, -0.0295],\n",
      "          [ 0.0162, -0.0018, -0.0005],\n",
      "          [-0.0102, -0.0026,  0.0149]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0152,  0.0196,  0.0274],\n",
      "          [-0.0225,  0.0240,  0.0202],\n",
      "          [-0.0373, -0.0292, -0.0124]],\n",
      "\n",
      "         [[ 0.0073,  0.0414,  0.0349],\n",
      "          [-0.0236, -0.0188,  0.0079],\n",
      "          [-0.0297,  0.0110, -0.0119]],\n",
      "\n",
      "         [[ 0.0231, -0.0061,  0.0180],\n",
      "          [ 0.0311,  0.0282,  0.0246],\n",
      "          [ 0.0368, -0.0168,  0.0064]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028, -0.0391,  0.0089],\n",
      "          [ 0.0182,  0.0133,  0.0367],\n",
      "          [-0.0401,  0.0269, -0.0413]],\n",
      "\n",
      "         [[-0.0072, -0.0360,  0.0163],\n",
      "          [ 0.0324,  0.0040, -0.0166],\n",
      "          [ 0.0131,  0.0220,  0.0082]],\n",
      "\n",
      "         [[ 0.0270,  0.0138, -0.0027],\n",
      "          [-0.0185, -0.0001,  0.0046],\n",
      "          [-0.0396,  0.0272,  0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0253, -0.0360,  0.0345],\n",
      "          [ 0.0360,  0.0215,  0.0391],\n",
      "          [ 0.0313,  0.0210, -0.0007]],\n",
      "\n",
      "         [[ 0.0169,  0.0402,  0.0335],\n",
      "          [ 0.0014,  0.0306, -0.0100],\n",
      "          [ 0.0171, -0.0179, -0.0238]],\n",
      "\n",
      "         [[-0.0146, -0.0021, -0.0268],\n",
      "          [-0.0021, -0.0244,  0.0154],\n",
      "          [ 0.0049,  0.0347, -0.0102]]]], requires_grad=True) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[ 2.0713e-02, -3.5395e-02, -2.6549e-02],\n",
      "          [ 1.5021e-02,  3.0317e-02, -2.9427e-02],\n",
      "          [-2.7968e-02, -1.1334e-02, -1.9573e-02]],\n",
      "\n",
      "         [[ 2.0719e-02,  2.4701e-02, -1.8366e-02],\n",
      "          [-2.0003e-03,  3.4704e-03, -3.3632e-02],\n",
      "          [ 9.7110e-03,  2.6124e-02, -1.4423e-02]],\n",
      "\n",
      "         [[-1.8319e-02,  1.8793e-02,  3.9057e-02],\n",
      "          [ 2.6284e-02, -3.1532e-03, -3.3062e-02],\n",
      "          [ 6.5800e-03, -1.4041e-02, -1.4870e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2747e-02,  1.6824e-02, -1.1291e-02],\n",
      "          [-1.0031e-02,  2.7852e-02, -6.2242e-03],\n",
      "          [ 1.7605e-02, -1.1342e-02, -2.4319e-02]],\n",
      "\n",
      "         [[-1.8419e-02,  7.4878e-07,  3.4321e-02],\n",
      "          [-1.9051e-02,  1.4680e-02,  3.6156e-02],\n",
      "          [-3.9567e-02, -9.7557e-03, -2.5681e-02]],\n",
      "\n",
      "         [[-3.3697e-02,  3.1713e-02, -2.7976e-02],\n",
      "          [-2.2465e-02,  1.3767e-02, -3.2984e-02],\n",
      "          [ 3.6652e-02, -3.1243e-02,  3.7123e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2168e-02,  9.1541e-03, -2.1859e-02],\n",
      "          [-2.2518e-02, -1.8782e-02, -9.7352e-03],\n",
      "          [-2.2328e-02, -3.0359e-02,  7.5874e-03]],\n",
      "\n",
      "         [[-2.7125e-02, -2.3543e-02,  2.3144e-02],\n",
      "          [-3.7459e-02, -2.2815e-03, -3.2500e-02],\n",
      "          [ 2.9518e-02, -2.9403e-02, -2.9963e-02]],\n",
      "\n",
      "         [[-2.3722e-02, -2.5402e-02,  3.2966e-02],\n",
      "          [ 3.5330e-02,  1.2251e-03, -2.7176e-02],\n",
      "          [ 2.8093e-02, -9.7601e-03,  2.3773e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2736e-02,  2.6912e-02,  2.0392e-02],\n",
      "          [-5.7418e-03, -6.2165e-03,  7.2896e-03],\n",
      "          [ 5.5417e-04,  2.0431e-02, -9.5828e-03]],\n",
      "\n",
      "         [[-1.8955e-02,  1.2041e-02,  2.7277e-02],\n",
      "          [ 3.6837e-03,  4.1227e-02,  1.3517e-02],\n",
      "          [ 3.8643e-02,  4.1444e-02,  4.7305e-03]],\n",
      "\n",
      "         [[-5.8033e-03, -2.3971e-02, -3.6453e-02],\n",
      "          [ 2.3090e-03,  2.1101e-03,  1.5795e-02],\n",
      "          [-2.8384e-02, -2.0505e-02, -3.4533e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.3113e-03, -2.9957e-02,  2.0808e-03],\n",
      "          [ 3.9787e-03,  2.2643e-02, -1.5987e-02],\n",
      "          [-1.9489e-02, -1.9018e-02, -3.4181e-02]],\n",
      "\n",
      "         [[-2.7769e-02,  2.5238e-02, -8.5870e-03],\n",
      "          [ 3.9879e-02,  2.6581e-02, -2.0951e-02],\n",
      "          [-1.7611e-02,  7.2277e-03,  3.7167e-02]],\n",
      "\n",
      "         [[ 3.8809e-02,  3.6904e-02,  3.2939e-02],\n",
      "          [-2.3328e-02,  1.0179e-02,  2.7650e-02],\n",
      "          [ 2.0741e-02, -2.3908e-02,  1.0761e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.6572e-03, -1.5039e-02,  3.2305e-02],\n",
      "          [ 2.9707e-02, -1.4633e-03, -2.4377e-02],\n",
      "          [-1.0857e-02,  2.1694e-02,  3.6459e-02]],\n",
      "\n",
      "         [[ 3.1117e-02,  3.4156e-02,  9.1682e-03],\n",
      "          [ 1.7007e-02,  4.0981e-02,  1.4621e-02],\n",
      "          [ 2.3553e-02,  1.5517e-03,  2.4596e-02]],\n",
      "\n",
      "         [[-2.4108e-02,  3.0603e-02,  7.8704e-03],\n",
      "          [-1.9209e-02, -1.0813e-02, -3.8618e-02],\n",
      "          [ 1.9556e-02, -1.6610e-02,  9.6093e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1987e-02,  3.6105e-02,  3.4813e-02],\n",
      "          [-2.2516e-02, -7.6243e-03,  5.4406e-03],\n",
      "          [ 2.7578e-02,  3.3225e-02,  3.2095e-02]],\n",
      "\n",
      "         [[-3.4663e-02,  5.5199e-03, -2.5790e-02],\n",
      "          [-1.1077e-02,  1.3289e-02, -3.7723e-02],\n",
      "          [ 3.8383e-02,  2.2158e-02,  7.2792e-05]],\n",
      "\n",
      "         [[-1.3162e-02, -1.4441e-02, -8.4098e-03],\n",
      "          [ 3.5304e-02,  2.0017e-02,  3.8267e-02],\n",
      "          [ 3.7190e-02, -3.2668e-02, -3.5712e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2446e-02,  2.7462e-02, -1.7750e-02],\n",
      "          [ 1.7388e-02, -1.3253e-02,  3.3402e-02],\n",
      "          [ 3.0198e-02,  2.0661e-02, -2.8895e-02]],\n",
      "\n",
      "         [[ 3.5037e-03, -4.4606e-03, -1.8141e-02],\n",
      "          [ 5.1584e-03, -1.1251e-02, -3.3280e-02],\n",
      "          [ 1.7314e-02,  3.6875e-02,  1.4748e-02]],\n",
      "\n",
      "         [[ 2.2866e-02, -1.3621e-02,  4.7431e-03],\n",
      "          [ 2.6961e-02,  2.7434e-02, -1.3583e-04],\n",
      "          [-1.7644e-02,  2.6745e-02,  1.6867e-02]]]], requires_grad=True) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[ 2.2444e-02, -1.4971e-03,  1.9627e-02],\n",
      "          [-1.5773e-02, -1.7390e-02,  3.8819e-03],\n",
      "          [-3.2163e-02,  2.7397e-02, -1.3458e-02]],\n",
      "\n",
      "         [[ 9.3810e-03,  4.9725e-04,  1.8649e-02],\n",
      "          [ 4.9914e-03, -4.0478e-02,  9.0481e-03],\n",
      "          [-8.5279e-03,  2.4837e-03,  2.2054e-02]],\n",
      "\n",
      "         [[-2.5673e-02, -1.9945e-02,  1.9840e-02],\n",
      "          [-2.7281e-02,  3.3466e-02, -2.7310e-02],\n",
      "          [ 2.9241e-02,  4.9708e-03,  3.8619e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1460e-02,  1.3926e-02,  2.6174e-03],\n",
      "          [ 1.6086e-02, -1.2000e-04, -4.0117e-02],\n",
      "          [-1.2852e-02,  1.6202e-02, -4.0600e-02]],\n",
      "\n",
      "         [[ 3.4191e-02, -1.2895e-02, -1.8202e-02],\n",
      "          [-4.2584e-03,  4.6865e-04, -7.4921e-03],\n",
      "          [-2.7622e-02, -2.1526e-02, -2.8721e-03]],\n",
      "\n",
      "         [[-9.3147e-04, -5.7251e-03,  3.3136e-02],\n",
      "          [ 6.5945e-04, -3.1469e-02,  1.5950e-02],\n",
      "          [-2.8247e-02, -1.1995e-02,  1.7523e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.2456e-02,  2.6485e-02,  3.6670e-02],\n",
      "          [ 2.2845e-02,  2.8652e-02, -3.1392e-03],\n",
      "          [-3.8608e-03, -3.6531e-02, -2.7346e-03]],\n",
      "\n",
      "         [[ 1.0984e-02, -3.3146e-02,  2.9151e-02],\n",
      "          [-7.0719e-03,  3.6172e-02, -1.0701e-02],\n",
      "          [ 2.2913e-02, -9.4290e-03,  2.9822e-02]],\n",
      "\n",
      "         [[-2.2394e-02,  6.5249e-03, -1.1260e-02],\n",
      "          [ 5.0101e-03, -3.6401e-04, -1.0822e-02],\n",
      "          [ 1.5123e-02, -1.7532e-02, -3.4860e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5287e-02, -2.6290e-02, -2.5540e-02],\n",
      "          [ 1.1104e-02, -4.5770e-03,  4.0396e-02],\n",
      "          [ 4.0580e-02,  1.5361e-02, -3.2079e-03]],\n",
      "\n",
      "         [[ 3.3557e-02, -3.8493e-02,  4.1298e-02],\n",
      "          [ 4.0633e-02,  3.2782e-02, -3.6130e-02],\n",
      "          [-2.7969e-02,  3.3621e-02,  4.0038e-02]],\n",
      "\n",
      "         [[-2.8347e-02, -1.1644e-02,  2.6577e-02],\n",
      "          [ 2.0796e-02,  2.2504e-02,  2.6329e-02],\n",
      "          [-2.2797e-03, -3.3287e-02, -3.3630e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3443e-02, -1.1028e-02, -2.9145e-02],\n",
      "          [-3.0164e-02, -3.7632e-02, -2.6247e-02],\n",
      "          [ 3.4080e-02, -2.9688e-02, -1.3248e-02]],\n",
      "\n",
      "         [[-2.9859e-03, -2.8862e-02,  1.4289e-02],\n",
      "          [-1.4489e-02,  1.9577e-02, -3.0291e-02],\n",
      "          [ 7.6656e-04,  3.4627e-02, -3.4194e-02]],\n",
      "\n",
      "         [[-3.3002e-02,  2.5397e-02, -2.5074e-02],\n",
      "          [ 2.6228e-02, -3.0102e-02,  8.8120e-03],\n",
      "          [-2.3786e-02, -2.5155e-02, -3.8072e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7986e-02, -3.4135e-02, -1.7246e-03],\n",
      "          [ 4.1480e-02, -3.1412e-02,  6.0724e-03],\n",
      "          [ 1.5807e-02,  1.8027e-02, -1.0721e-02]],\n",
      "\n",
      "         [[ 6.8059e-03, -3.1170e-05, -1.0637e-02],\n",
      "          [-1.6099e-02, -3.2197e-02,  3.5099e-02],\n",
      "          [-1.6032e-02, -1.0197e-02, -8.0725e-03]],\n",
      "\n",
      "         [[ 2.7442e-02, -2.8132e-02,  3.1853e-02],\n",
      "          [ 2.9682e-02, -1.3423e-02,  5.7817e-03],\n",
      "          [ 3.3791e-02, -4.4313e-03,  3.0005e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4017e-02,  4.0313e-02, -3.0878e-02],\n",
      "          [-6.2924e-03, -2.0983e-02,  1.7695e-02],\n",
      "          [-2.0561e-02,  5.9569e-03, -1.6288e-02]],\n",
      "\n",
      "         [[-3.6318e-02,  2.4402e-03,  3.9217e-02],\n",
      "          [ 3.7148e-02,  3.8315e-02, -1.5103e-02],\n",
      "          [ 1.0274e-02, -1.7718e-02,  2.6448e-02]],\n",
      "\n",
      "         [[-3.9683e-03, -9.6811e-03,  3.9040e-02],\n",
      "          [ 1.6772e-02,  2.3047e-02, -2.7454e-02],\n",
      "          [-9.0685e-05,  4.0969e-02, -1.9932e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.7160e-03, -1.5079e-02,  3.1791e-02],\n",
      "          [-1.1750e-02,  1.7384e-02,  1.6113e-02],\n",
      "          [ 3.3138e-02, -4.0911e-02,  2.5307e-03]],\n",
      "\n",
      "         [[-4.0366e-02, -3.0819e-02,  4.0949e-02],\n",
      "          [ 1.2814e-02,  1.0650e-02,  1.0768e-02],\n",
      "          [ 1.2342e-03, -1.9698e-02,  8.2982e-03]],\n",
      "\n",
      "         [[-3.4866e-02,  1.6260e-02, -2.4839e-02],\n",
      "          [ 3.7244e-02,  3.8665e-02,  1.6780e-02],\n",
      "          [-2.3360e-02,  2.4168e-02,  3.3163e-02]]]], requires_grad=True) \n"
     ]
    }
   ],
   "source": [
    "# setting the weights to 1\n",
    "conv_all_one = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    #conv.weight = nn.Parameter(torch.ones(conv.weight.shape))\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_all_one.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy input has chape torch.Size([1, 64, 1, 1])\n",
      "tensor([[[[ 0.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 2.]],\n",
      "\n",
      "         [[ 3.]],\n",
      "\n",
      "         [[ 4.]],\n",
      "\n",
      "         [[ 5.]],\n",
      "\n",
      "         [[ 6.]],\n",
      "\n",
      "         [[ 7.]],\n",
      "\n",
      "         [[ 8.]],\n",
      "\n",
      "         [[ 9.]],\n",
      "\n",
      "         [[10.]],\n",
      "\n",
      "         [[11.]],\n",
      "\n",
      "         [[12.]],\n",
      "\n",
      "         [[13.]],\n",
      "\n",
      "         [[14.]],\n",
      "\n",
      "         [[15.]],\n",
      "\n",
      "         [[16.]],\n",
      "\n",
      "         [[17.]],\n",
      "\n",
      "         [[18.]],\n",
      "\n",
      "         [[19.]],\n",
      "\n",
      "         [[20.]],\n",
      "\n",
      "         [[21.]],\n",
      "\n",
      "         [[22.]],\n",
      "\n",
      "         [[23.]],\n",
      "\n",
      "         [[24.]],\n",
      "\n",
      "         [[25.]],\n",
      "\n",
      "         [[26.]],\n",
      "\n",
      "         [[27.]],\n",
      "\n",
      "         [[28.]],\n",
      "\n",
      "         [[29.]],\n",
      "\n",
      "         [[30.]],\n",
      "\n",
      "         [[31.]],\n",
      "\n",
      "         [[32.]],\n",
      "\n",
      "         [[33.]],\n",
      "\n",
      "         [[34.]],\n",
      "\n",
      "         [[35.]],\n",
      "\n",
      "         [[36.]],\n",
      "\n",
      "         [[37.]],\n",
      "\n",
      "         [[38.]],\n",
      "\n",
      "         [[39.]],\n",
      "\n",
      "         [[40.]],\n",
      "\n",
      "         [[41.]],\n",
      "\n",
      "         [[42.]],\n",
      "\n",
      "         [[43.]],\n",
      "\n",
      "         [[44.]],\n",
      "\n",
      "         [[45.]],\n",
      "\n",
      "         [[46.]],\n",
      "\n",
      "         [[47.]],\n",
      "\n",
      "         [[48.]],\n",
      "\n",
      "         [[49.]],\n",
      "\n",
      "         [[50.]],\n",
      "\n",
      "         [[51.]],\n",
      "\n",
      "         [[52.]],\n",
      "\n",
      "         [[53.]],\n",
      "\n",
      "         [[54.]],\n",
      "\n",
      "         [[55.]],\n",
      "\n",
      "         [[56.]],\n",
      "\n",
      "         [[57.]],\n",
      "\n",
      "         [[58.]],\n",
      "\n",
      "         [[59.]],\n",
      "\n",
      "         [[60.]],\n",
      "\n",
      "         [[61.]],\n",
      "\n",
      "         [[62.]],\n",
      "\n",
      "         [[63.]]]])\n"
     ]
    }
   ],
   "source": [
    "# defining a dummy input of shape (1 (batch_size), channels in, 1, 1)\n",
    "arr = np.arange(channels_in).reshape(1,channels_in, 1, 1)\n",
    "dummy = torch.tensor(arr, dtype=torch.float)\n",
    "print(f\"dummy input has chape {dummy.shape}\")\n",
    "print(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do the forward pass with our 3 cnn layers\n",
    "def compute_result(dummy, dilations):\n",
    "    results = []\n",
    "    for dil in dilations:\n",
    "        results.append(dil(dummy))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-5.9458]],\n",
      "\n",
      "         [[ 1.9389]],\n",
      "\n",
      "         [[-1.7697]],\n",
      "\n",
      "         [[ 3.9780]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[ 2.8810]],\n",
      "\n",
      "         [[-1.8030]],\n",
      "\n",
      "         [[12.4181]],\n",
      "\n",
      "         [[ 1.8972]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[ 4.7673]],\n",
      "\n",
      "         [[-0.8068]],\n",
      "\n",
      "         [[10.0455]],\n",
      "\n",
      "         [[ 9.6450]]]], grad_fn=<ThnnConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_all_one = compute_result(dummy, conv_all_one)\n",
    "print(results_all_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now sett all weights to 0.1 the output will just be a tensor of shape (1,4,1,1) with all values being 0.1 * 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n"
     ]
    }
   ],
   "source": [
    "conv_all_point_one = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    conv.weight = nn.Parameter(torch.ones(conv.weight.shape))\n",
    "    conv.weight[:] = 0.1\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_all_point_one.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<ThnnConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_all_point_one = compute_result(dummy, conv_all_point_one)\n",
    "print(results_all_point_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n"
     ]
    }
   ],
   "source": [
    "# now we set all weights to 0.1 except the one in the iddle which will be 1\n",
    "conv_one_one = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    conv.weight[:] = 0.1\n",
    "    # the middle weight determines the output\n",
    "    # 1 -> 2016\n",
    "    conv.weight[:,:,1,1] = 1\n",
    "    # everything else will not matter\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_one_one.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<ThnnConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_one_one = compute_result(dummy, conv_one_one)\n",
    "print(results_one_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact we can choos all the weights we want for the non-center weights of our kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 3.1416e+00]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 3.1416e+00]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 3.1416e+00]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]]], grad_fn=<CopySlices>) \n"
     ]
    }
   ],
   "source": [
    "# now we set all weights to 0.1 except the one in the iddle which will be 1\n",
    "conv_one_one_rest_random = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    conv.weight[:] = 0.1\n",
    "    # the middle weight determines the output\n",
    "    # 1 -> 2016\n",
    "    conv.weight[:,:,1,1] = 1\n",
    "    # everything else will not matter\n",
    "    conv.weight[:,0,2,2] = 10000000000000000\n",
    "    conv.weight[2,1,0,0] = 0\n",
    "    conv.weight[1,1,2,2] = np.pi\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_one_one_rest_random.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<ThnnConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<ThnnConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_one_one_rest_random = compute_result(dummy, conv_one_one_rest_random)\n",
    "print(results_one_one_rest_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each output has the same dimensions and the same values.\n",
    "Morover, only changing the dilation and the padding value will render all weights of our dilation useless except the one in the middle which will be multiplied with our 1x1 input.\n",
    "    We can demonstrate that by setting all weights to 0 except the one in the middle. Thus we could achive the same when using convolutions with 1x1 kernels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False) weights: torch.Size([4, 64, 1, 1]) \n",
      "Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False) weights: torch.Size([4, 64, 1, 1]) \n",
      "Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False) weights: torch.Size([4, 64, 1, 1]) \n"
     ]
    }
   ],
   "source": [
    "channels_in = 64\n",
    "reduction = 16\n",
    "channels_out = channels_in // reduction\n",
    "simple_layers = []\n",
    "for i in range(3):\n",
    "    simple_layers.append(nn.Conv2d(channels_in, channels_out, 1, bias=False))\n",
    "    simple_layers[i].weight = nn.Parameter(torch.ones(simple_layers[i].weight.shape))\n",
    "    simple_layers[i].weight[:] = 0.1 \n",
    "    print(f\"{simple_layers[i]} weights: {simple_layers[i].weight.shape} \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_result(dummy, simple_layers)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
